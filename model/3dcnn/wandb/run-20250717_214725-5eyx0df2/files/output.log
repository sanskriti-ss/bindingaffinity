/root/miniconda3/envs/fetchai/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-1/258] training, loss: 9.652, lr: 0.0007000
checkpoint saved: /home/karen/Projects/FAST/models/refined_3dcnn_model.pth
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-2/258] training, loss: 8.145, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-3/258] training, loss: 4.418, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-4/258] training, loss: 4.526, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-5/258] training, loss: 6.090, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-6/258] training, loss: 5.982, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-7/258] training, loss: 4.663, lr: 0.0007000
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 64, 24, 24, 24])
torch.Size([16, 128, 8, 8, 8])
torch.Size([16, 128, 4, 4, 4])
torch.Size([16, 256, 2, 2, 2])
torch.Size([16, 2048])
torch.Size([16, 100])
torch.Size([16, 1])
[1/50-8/258] training, loss: 4.852, lr: 0.0007000
Traceback (most recent call last):
  File "/home/karen/Projects/FAST/model/3dcnn/main_train.py", line 350, in <module>
    main()
  File "/home/karen/Projects/FAST/model/3dcnn/main_train.py", line 347, in main
    train()
  File "/home/karen/Projects/FAST/model/3dcnn/main_train.py", line 210, in train
    vol_batch = gaussian_filter(x_batch)
  File "/root/miniconda3/envs/fetchai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fetchai/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/karen/Projects/FAST/model/3dcnn/img_util.py", line 204, in forward
    return self.conv(input, weight=self.weight, groups=self.groups, padding=self.padding)
KeyboardInterrupt

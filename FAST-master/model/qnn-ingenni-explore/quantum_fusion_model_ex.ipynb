{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b47e06f5",
   "metadata": {},
   "source": [
    "# Quantum Fusion model\n",
    "\n",
    "This notebook designs a quantum fusion model, designed to efficiently integrate the extracted features from two classical neural network models to produce enhanced predictions. The proposed model strategically integrates 3D-CNNs and SG-CNNs to leverage their respective strengths in processing diverse facets of the training data. \n",
    "The simulation results presented here will demonstrate the superior performance of the quantum fusion model relative to state-of-the-art classical models. Particularly, the proposed model achieves a 6\\% improvement in the prediction accuracy, and exhibits faster, smoother, and more stable convergence, thereby boosting its generalization capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1518391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bda3cf4",
   "metadata": {},
   "source": [
    "## Problem setting: Binding affinity predictions\n",
    "\n",
    "In the field of drug discovery, it is imperative to identify proteins that are instrumental in the cascade of molecular interactions leading to a specific disease. Upon the identification of such a target protein, a list of prospective drug candidates is generated. These candidates, often described as small molecules or compounds termed **ligands**, have the potential to modulate the target protein's activity through binding interactions. Ideal ligands are chosen based on their high binding affinity to the target protein, coupled with minimal off-target interactions with other proteins. However, quantifying such binding affinities is a resource-intensive endeavor, both in terms of time and financial investment. This is particularly true considering that the initial screening process often encompasses thousands of compounds\n",
    "\n",
    "The transition from conventional laboratory methods to computer-aided design has markedly improved the efficiency and accuracy of drug discovery and binding affinity prediction.  Quantum machine learning  models, in particular, are well-suited to manage the challenges of exponentially increasing data dimensionality, often outperforming traditional ML models under specific conditions. Taken together, these technological advancements make QML and hybrid quantum-classical models highly promising for navigating the complex, high-dimensional challenges intrinsic to drug discovery.\n",
    "\n",
    "The main contribution of this notebook is the development of a novel quantum fusion model aimed at enhancing the binding affinity predictions in drug discovery. \n",
    "\n",
    "To demonstrate the power of this approach, the first step is to load the training, valiudation and test data.\n",
    "The data in this case is the extracted features from the 3D CNN (6 features) and the ones from the Spatial Graph CNN (10 features), resulting in a vector of 16 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b7e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to improve run time and prevent displaying warning errors which can be ignored\n",
    "import torch\n",
    "\n",
    "# from ingenii_quantum.hybrid_networks.layers import QuantumFCLayer\n",
    "\n",
    "from layers import QuantumFCLayer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
    "from scipy.stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23f84ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples training:  4779  Features size:  16\n",
      "SHAPE: Train fc16 Features:  (4779, 16) Train y: (4779,)\n",
      "------------------------------------------------------\n",
      "Number of samples validation:  537  Features size:  16\n",
      "SHAPE: Val Features:  (537, 16) Validation:  (537,)\n",
      "------------------------------------------------------\n",
      "Number of samples test:  285  Features size:  16\n",
      "SHAPE:  Test Features:  (285, 16) Testing:  (285,)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "\n",
    "# Load features\n",
    "fc_6_train=np.array(np.load('data/refined_train_fc6.npy'))\n",
    "fc_10_train=np.array(np.load('data/refined_train_fc10.npy'))\n",
    "# Concatenate features\n",
    "fc_16_train = np.concatenate((fc_6_train, fc_10_train), axis=1)\n",
    "\n",
    "# Load prediction\n",
    "y_vals_train = np.genfromtxt(\"data/refined_train.csv\", dtype=float, delimiter=',', names=True) \n",
    "y_train = np.array([ys[1] for ys in y_vals_train])\n",
    "y_classical_train = np.array([ys[2] for ys in y_vals_train])\n",
    "print('Number of samples training: ', y_train.shape[0], ' Features size: ', fc_16_train.shape[1])\n",
    "print('SHAPE:', 'Train fc16 Features: ', fc_16_train.shape,'Train y:', y_train.shape, )\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# VALIDATION\n",
    "# Load features\n",
    "fc_6_val=np.array(np.load('data/refined_val_fc6.npy'))\n",
    "fc_10_val=np.array(np.load('data/refined_val_fc10.npy'))\n",
    "# Concatenate features\n",
    "fc_16_val = np.concatenate((fc_6_val, fc_10_val), axis=1)\n",
    "\n",
    "# Load prediction\n",
    "y_vals_val = np.genfromtxt(\"data/refined_val.csv\", dtype=float, delimiter=',', names=True) \n",
    "y_val = np.array([ys[1] for ys in y_vals_val])\n",
    "y_classical_val = np.array([ys[2] for ys in y_vals_val])\n",
    "\n",
    "print('Number of samples validation: ', y_val.shape[0], ' Features size: ', fc_16_val.shape[1])\n",
    "print('SHAPE:', 'Val Features: ', fc_16_val.shape, 'Validation: ', y_val.shape )\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# TEST\n",
    "# Load features\n",
    "fc_6_test=np.array(np.load('data/core_test_fc6.npy'))\n",
    "fc_10_test=np.array(np.load('data/core_test_fc10.npy'))\n",
    "# Concatenate features\n",
    "fc_16_test = np.concatenate((fc_6_test, fc_10_test), axis=1)\n",
    "\n",
    "# Load prediction\n",
    "y_vals_test = np.genfromtxt(\"data/core_test.csv\", dtype=float, delimiter=',', names=True) \n",
    "y_test = np.array([ys[1] for ys in y_vals_test])\n",
    "y_classical_test = np.array([ys[2] for ys in y_vals_test])\n",
    "print('Number of samples test: ', y_test.shape[0], ' Features size: ', fc_16_test.shape[1])\n",
    "print('SHAPE: ', 'Test Features: ', fc_16_test.shape,'Testing: ', y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a118a6d",
   "metadata": {},
   "source": [
    "## Scale data\n",
    "\n",
    "We will scale the data to the [0,1] domain so that it is suitable for the quantum neural network with amplitude encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5211a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SHAPE:X train: (4779, 16), y train scaled: (4779, 1)\n"
     ]
    }
   ],
   "source": [
    "#model parameters using aplitude encoding\n",
    "nqbits = 4\n",
    "length = 2**nqbits\n",
    "\n",
    "# Training data\n",
    "X_train = np.array(fc_16_train)\n",
    "# Scale y values\n",
    "scaler = MinMaxScaler((0,1))\n",
    "y_train_scaled = scaler.fit_transform(y_train.reshape(-1,1)).astype(np.float32)\n",
    "print(f\" SHAPE:X train: {X_train.shape}, y train scaled: {y_train_scaled.shape}\")\n",
    "\n",
    "# Validation data\n",
    "X_val = np.array(fc_16_val)\n",
    "y_val_scaled = scaler.transform(y_val.reshape(-1,1)).astype(np.float32)\n",
    "# Test data\n",
    "X_test = np.array(fc_16_test)\n",
    "y_test_scaled = scaler.transform(y_test.reshape(-1,1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb23058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6878be08",
   "metadata": {},
   "source": [
    "## Quantum Neural Network\n",
    "\n",
    "We will use Ingenii's library to design a quantum neural network to fuse the features from the two CNNs. Since our input data has size 16, we will use amplitude encoding to reduce the number of qubits needed from the neural network. We will use the first option of ansatz, which consists of a set of strongly entangling layers. The depth of the neural network is a hyperparameter that we can tune. In this example, we will use a depth of 10 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231badd2",
   "metadata": {},
   "source": [
    "## Define quantum layer\n",
    "\n",
    "We define the quantum layer with Igenii's library. We will use Keras to train the hybridd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35d5a5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = ['ZIII','IZII','IIZI', 'IIIZ'] # The observables will be the expected value of the Z operators in each qubit\n",
    "input_size= 16\n",
    "n_layers = 10\n",
    "# Initializing the class\n",
    "qnn = QuantumFCLayer(\n",
    "    input_size=input_size, n_layers=n_layers, encoding='amplitude',\n",
    "    ansatz=4, observables=obs, backend='default.qubit')\n",
    "\n",
    "# Create the quantum layer\n",
    "qnn_layer = qnn.create_layer(type_layer='torch')\n",
    "\n",
    "# We test the prediction of the quantum layer\n",
    "x = torch.randn(2, input_size)\n",
    "\n",
    "qnn_layer(x).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643f40c",
   "metadata": {},
   "source": [
    "## Define the whole model\n",
    "\n",
    "We add a classical dense layer to output the final prediction. The ReLu activation function is used. We compile the resulting model with Adam optimizer and a learning rate of $\\eta=0.002$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc7b769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchModel, self).__init__()\n",
    "        self.qlayer = qnn_layer\n",
    "        self.clayer = torch.nn.Linear(4, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.qlayer(x).double()\n",
    "        # print(\"forwrd: x_shape\", x.shape)\n",
    "        # print(\"fwd x type\", type(x))\n",
    "        x = self.clayer(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "model = TorchModel()\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of trainable parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260383a",
   "metadata": {},
   "source": [
    "Notice that the model has 125 training parameters, including 120 quantum parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b36624",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d66325",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X_train, y_train_scaled)), batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X_val, y_val_scaled)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(X_test, y_test_scaled)), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0609c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "    batch=0\n",
    "    for xs, ys in training_loader:\n",
    "        opt.zero_grad()\n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        loss_evaluated.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        training_losses.append(loss_evaluated.detach())\n",
    "        batch+=1\n",
    "\n",
    "    avg_loss = np.mean(training_losses)\n",
    "    train_loss.append(avg_loss)\n",
    "    print(\"Training loss over epoch {}: {:.4f}\".format(epoch + 1, avg_loss))\n",
    "\n",
    "    for xs, ys in val_loader:\n",
    "        \n",
    "        loss_evaluated = loss(model(xs), ys)\n",
    "        val_losses.append(loss_evaluated.detach())\n",
    "\n",
    "    avg_loss = np.mean(val_losses)\n",
    "    val_loss.append(avg_loss)\n",
    "    print(\"Validation loss over epoch {}: {:.4f}\\n\".format(epoch + 1, avg_loss))\n",
    "\n",
    "end = time.time()\n",
    "print('Training time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affddee",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Now we test the model's accuracy and compare it with the classical counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd2f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model(torch.tensor(X_test)).detach().numpy()\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred)\n",
    "print('QUANTUM FUSION')\n",
    "print('Test MSE is', mean_squared_error(y_test,y_test_pred))\n",
    "print('Test MAE is', mean_absolute_error(y_test,y_test_pred))\n",
    "print('Test R2 is', r2_score(y_test,y_test_pred))\n",
    "print('Test spearman is', spearmanr(y_test,y_test_pred))\n",
    "\n",
    "print('\\nCLASSICAL FUSION')\n",
    "print('Test MSE is', mean_squared_error(y_test,y_classical_test))\n",
    "print('Test MAE is', mean_absolute_error(y_test,y_classical_test))\n",
    "print('Test R2 is', r2_score(y_test,y_classical_test))\n",
    "print('Test spearman is', spearmanr(y_test,y_classical_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test)\n",
    "plt.plot(y_test_pred)\n",
    "plt.ylim(0, np.max(y_test)*1.3)\n",
    "plt.legend(['Actual','predicted'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f9562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fetch-qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
